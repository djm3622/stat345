{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31237709",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STAT 345: Nonparametric Statistics\n",
    "\n",
    "## Lesson 05.2: The Kruskal-Wallis Test\n",
    "\n",
    "**Reading: Conover Section 5.2**\n",
    "\n",
    "*Prof. John T. Whelan*\n",
    "\n",
    "Tuesday 25 February 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d755e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These lecture slides are in a computational notebook.  You have access to them through http://vmware.rit.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a2941",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Flat HTML and slideshow versions are also in MyCourses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb292b73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The notebook can run Python commands (other notebooks can use R or Julia; \"Ju-Pyt-R\").  Think: computational data analysis, not \"coding\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b9567",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Standard commands to activate inline interface and import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6081e979",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e27d43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0,5.0)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b39835",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall Wilcoxon rank-sum test for two independent samples $\\{x_{1j}|j=1,\\ldots,n_1\\}$ & $\\{x_{2j}|j=1,\\ldots,n_2\\}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1f6fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Test whether they could be from same distribution by ranking all $N=n_1+n_2$ data points.  If sum of the $\\{R_{1j}\\}$ (which we've called $W$) is too high or too low, we reject $H_0$ (in a two-sided test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d930f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since $\\sum_{j=1}^{n_2} R_{2j}=\\frac{N(N+1)}{2}-\\sum_{j=1}^{n_1}R_{1j}$, one rank-sum carries all the information.  Two-sided test asks if $\\sum_{j=1}^{n_1}R_{1j}$ is too far from its expected value of $n_1\\frac{N+1}{2}$ (which would mean $\\sum_{j=1}^{n_2}R_{2j}$ is too far from $n_2\\frac{N+1}{2}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88c5d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now generalize: suppose instead of two, we have $k$ samples, and let the $i$th sample $\\{x_{ij}\\}$ have size $n_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5dbd37c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i_j = [np.array([ 14.97,   5.80,  25.03,   5.50 ]),\n",
    "       np.array([  5.83,  13.96,  21.96]),\n",
    "       np.array([ 17.89,  23.03,  61.09,   18.62,  55.51])]\n",
    "n_i = np.array([len(xi_j) for xi_j in x_i_j]); n_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbf3c72",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = len(n_i); k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf5b42",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that, since the sample sizes $\\{n_i\\}$ can all be different, we can't just store this in Python as an array with two indices.  Instead `x_i_j` is a list of $k$ arrays, which in general have different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ed8dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The total number of data points in all of the samples is\n",
    "$N=\\sum_{i=1}^k n_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8df3f72",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = np.sum(n_i); N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b9fbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We're trying to assess the null hypothesis that the samples were all drawn from the same distribution--or more specifically that they were drawn in a way that $P(\\color{royalblue}{X_{ij}}\\mathbin{>}\\color{royalblue}{X_{i'j'}}) = P(\\color{royalblue}{X_{ij}}\\mathbin{<}\\color{royalblue}{X_{i'j'}})$--against the alternative that some of the samples (not specified which) are drawn from distributions that have different location parameters, i.e., that $P(\\color{royalblue}{X_{ij}}\\mathbin{>}\\color{royalblue}{X_{i'j'}}) \\ne P(\\color{royalblue}{X_{ij}}\\mathbin{<}\\color{royalblue}{X_{i'j'}})$ for at least one pair $(i,i')$.  We wish to this in a rank-based way which does not rely on a particular assumed sampling distribution for the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c586a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We combine (\"concatenate\") all $N$ values into a single list and rank them, and let the\n",
    "rank of $x_{ij}$ be $R_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efbbe766",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.97,  5.8 , 25.03,  5.5 ,  5.83, 13.96, 21.96, 17.89, 23.03,\n",
       "       61.09, 18.62, 55.51])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_r = np.concatenate(x_i_j); x_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e718e31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  2., 10.,  1.,  3.,  4.,  8.,  6.,  9., 12.,  7., 11.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_r = stats.rankdata(x_r); R_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83ec200",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A little trick to make a list of labels to see which of the k samples each value is from:\n",
    "i_r = np.concatenate([(i,)*n_i[i] for i in range(k)]); i_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028711f6",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 5.,  2., 10.,  1.]),\n",
       " array([3., 4., 8.]),\n",
       " array([ 6.,  9., 12.,  7., 11.])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This lets us organize Rij into a list of arrays\n",
    "R_i_j = [R_r[i_r==i] for i in range(k)]; R_i_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e74f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we write the sum of the ranks in the\n",
    "$i$th sample as $R_{i}=\\sum_{j=1}^{n_i} R_{ij}=n_{i}\\overline{R}_i$, we\n",
    "have $k$ statistics $\\{R_{i}\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9bf314",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18., 15., 45.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_i = np.array([np.sum(Ri_j) for Ri_j in R_i_j]); R_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03764e0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However they can be described by only\n",
    "$k-1$ quantities, since they obey the constraint\n",
    "$\\sum_{i=1}^k R_{i}=\\frac{N(N+1)}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f16ab5c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.0, 78)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(R_i), (N*(N+1)//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c701a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For $k=2$, we\n",
    "already saw that the sum of the ranks in the second group, $R_2$, which\n",
    "we called $W_y$, was determined by the sum of the ranks in the first\n",
    "group, $R_1$, which we called $W_x$, so there was only $k-1=1$\n",
    "independent statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663e3bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we consider the $k$ statistics $\\{{\\color{royalblue}{R_i}}\\}$, they\n",
    "have expectation values, assuming the null hypothesis that the samples are equally likely to contain any combination of ranks, of\n",
    "$$E({\\color{royalblue}{R_i}}) = n_i\\frac{N+1}{2}=n_i \\overline{R}$$\n",
    "If the alternative hypothesis is true, and some of the samples will have a tendency to contain higher or lower ranks than the average, then some of the differences $\\{\\color{royalblue}{R_i}-n_i\\overline{R}\\}$ will be \"too far\" from zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ec7525",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.5, 6.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rbar = 0.5*(N+1); (Rbar,np.mean(R_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d3dc9ed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8. , -4.5, 12.5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_i - n_i * Rbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a1f6c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this sort of situation, where a multidimensional measurement could be off in any \"direction\" from its expected value, the standard approach is to convert this to a normalized \"distance\", whose value under the null hypothesis is described by a chi-squared distribution.  Not only the null distribution but the construction of the distance itself relies upon the normal approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c059c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder of $\\chi^2$ Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb77de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: if ${\\color{royalblue}{Z_i}}$ are $n$ independent standard normal random variables, then\n",
    "${\\color{royalblue}{W}} = \\sum_{i=1}^n ({\\color{royalblue}{Z_i}})^2$\n",
    "is a chi-squared random variable with $n$ degrees of freedom,\n",
    "${\\color{royalblue}{W}}\\sim\\chi^2(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc0342",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if\n",
    "$\\{{\\color{royalblue}{X_i}}\\}$ are independent normal random variables\n",
    "with $E({\\color{royalblue}{X_i}})=\\mu_i$ and\n",
    "$\\operatorname{Var}({\\color{royalblue}{X_i}})=\\sigma_i^2$, then\n",
    "$${\\color{royalblue}{W}} = \\sum_{i=1}^n \\left(\\frac{{\\color{royalblue}{X_i}}-\\mu_i}{\\sigma_i}\\right)^2\\sim\\chi^2(n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502dad0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Analogous situation where the rvs are not independent: suppose $\\{{\\color{royalblue}{X_i}}\\}$ is a random sample from $N(\\mu,\\sigma^2)$.\n",
    "$${\\color{royalblue}{Y_i}} = {\\color{royalblue}{X_i}} - {\\color{royalblue}{{{\\overline{X}}}}}\n",
    "  = {\\color{royalblue}{X_i}} - \\frac{1}{n}\\sum_{k=1}^n {\\color{royalblue}{X_k}}$$\n",
    "are $n$ correlated rvs related by one constraint $\\sum_{i=1}^n{\\color{royalblue}{Y_i}}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496cc13f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One\n",
    "of the results of Studentâ€™s theorem, which underpins the confidence\n",
    "intervals for mean and variance when both are unknown, is that\n",
    "$$\\sum_{i=1}^n \\left(\\frac{{\\color{royalblue}{Y_i}}}{\\sigma}\\right)^2\n",
    "  = \\sum_{i=1}^n \\left(\\frac{{\\color{royalblue}{X_i}} - {\\color{royalblue}{{{\\overline{X}}}}}}{\\sigma}\\right)^2\n",
    "  \\sim \\chi^2(n-1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb647a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For a derivation of this result, see e.g., section 6.3 of <https://ccrg.rit.edu/~whelan/courses/2015_3fa_STAT_405/notes03.pdf>.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fbcd7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The one constraint among the $n$ rvs\n",
    "$\\{{\\color{royalblue}{Y_i}}\\}$ causes the # of degrees of freedom\n",
    "to be $n-1$ rather than $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8bc1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Construction of Kruskall-Wallis Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9feea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We have $k-1$ independent quantities among the $k$ rank-sums $\\{\\color{royalblue}{R_i}=\\sum_{j=1}^{n_i}\\color{royalblue}{R_{ij}}\\}$, to be combined into a single $\\chi^2$-like statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060d265",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The means are $E({\\color{royalblue}{R_i}}) = n_i\\frac{N+1}{2}=n_i \\overline{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49155a33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If there are no ties, they have variances and covariances\n",
    "$$\\operatorname{Var}({\\color{royalblue}{R_i}}) = n_i(N-n_i)\\frac{N+1}{12}\n",
    "\\qquad\\operatorname{Cov}({\\color{royalblue}{R_i}},{\\color{royalblue}{R_{\\ell}}}) = -n_in_{\\ell}\\frac{N+1}{12}\n",
    "  \\qquad \\hbox{if } i\\ne \\ell$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef32a23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The variances and covariances can be\n",
    "summarized into a variance-covariance matrix with elements$$\n",
    "  \\operatorname{Cov}({\\color{royalblue}{R_i}},{\\color{royalblue}{R_{\\ell}}}) = (N\\delta_{i\\ell}n_i-n_in_{\\ell})\\frac{N+1}{12}\n",
    "  \\qquad  i=1,\\ldots k; \\quad\\ell=1,\\ldots k \n",
    "  $$\n",
    "where $$\\delta_{i\\ell}\n",
    "  =\n",
    "  \\begin{cases}\n",
    "    1 & \\hbox{if } i=\\ell \\\\\n",
    "    0 & \\hbox{if } i\\ne \\ell\n",
    "  \\end{cases}$$ is the Kronecker delta (the elements of the identity\n",
    "matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdb148",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We could turn any one of them into a standard normal by\n",
    "shifting and scaling, but instead we combine them into a single $\\chi^2$-like statistic\n",
    "$${\\color{royalblue}{T}} = \\frac{12}{N(N+1)}\n",
    "  \\sum_{i=1}^k \\frac{1}{n_i}\\left({\\color{royalblue}{R_i}}-n_i\\frac{N+1}{2}\\right)^2\n",
    "  \\sim \\chi^2(k-1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87118f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The key step in the derivation is to write the variance-covariance\n",
    "    matrix as $\\frac{N(N+1)}{12\\sqrt{n_i n_{\\ell}}}\n",
    "      \\left(\\delta_{ij}-\\frac{\\sqrt{n_i n_{\\ell}}}{N}\\right)$ and\n",
    "    recognize the expression in parentheses as a projection operator\n",
    "    with one zero eigenvalue and $k-1$ unit eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52e8e3fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.153846153846154"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 12/(N*(N+1)) * np.sum((R_i-n_i*Rbar)**2/n_i); T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f52ceb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We get the $p$-value by using the $\\chi^2$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf570406",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12531520484413722"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chi2(df=k-1).sf(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628161c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If there are ties, variance is reduced. Various equivalent forms;\n",
    "the simplest is probably\n",
    "$${\\color{royalblue}{T}} = (N-1)\n",
    "  \\frac{\\sum_{i=1}^k\\frac{1}{n_i}\\left({\\color{royalblue}{R_i}}-n_i\\overline{R}\\right)^2}\n",
    "  {\\sum_{i=1}^k \\sum_{j=1}^{n_i}\n",
    "    \\left({\\color{royalblue}{R_{ij}}}-\\overline{R}\\right)^2}\n",
    "  \\sim \\chi^2(k-1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667b3117",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.153846153846154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(N-1) * np.sum((R_i-n_i*Rbar)**2/n_i) / np.sum((R_r-Rbar)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a476d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Of course, this also works if there are no ties; in that case\n",
    "$\\sum_{i=1}^k \\sum_{j=1}^{n_i}\n",
    "    \\left({\\color{royalblue}{R_{ij}}}-\\overline{R}\\right)^2 = \\sum_{r=1}^N\n",
    "    \\left(R_r-\\overline{R}\\right)^2$ is just $\\frac{(N+1)N(N-1)}{12}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3362c86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143.0, 143.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((R_r-Rbar)**2), (N+1)*N*(N-1)/12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4633409",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<span id=\"fn3\">[<sup>3</sup>](#fm2)By the way, if you're familiar with the Analysis of Variance (ANOVA) this expression may look somewhat familar; see for example equation (1.6) of <https://ccrg.rit.edu/~whelan/courses/2019_1sp_MATH_252/notes10.pdf>.  In fact, the Kruskall-Wallis statistic is sometimes referred to as \"ANOVA with ranks\".  One key difference, though, is that the expression $\\sum_{i=1}^k \\sum_{j=1}^{n_i}\n",
    "    \\left({\\color{royalblue}{R_{ij}}}-\\frac{N+1}{2}\\right)^2$ which is known as the Total Sum of Squares (SST), is equal to $\\frac{(N+1)N(N-1)}{12}$ (if there are no ties, but in any event determined by the available ranks rather than their arrangement in the samples), while in ANOVA it is a linear combination of two $\\chi^2$ random variables (SST=SSTr+SSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aebe0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4a90a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Kruskall-Wallis test tells us whether some of the samples have differing location parameters, but it doesn't tell us which pairs of samples differ significantly, nor in which direction.  (It's inherently like a two-sided hypothesis test in this regard.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9e84c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Conover describes a procedure for identifying which differences are significant.  (This plays a role something like Tukey's multiple comparison test in the ANOVA case, except that it's more fundamentally connected to the original statistic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83854cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The test has become known as the Conover-Iman test, and while it's probably best preserved in your textbook itself, the original reference is Conover and Iman, \"\n",
    "Multiple-comparisons procedures. Informal report,\" Los Alamos Informal Report LA-7677-MS, https://doi.org/10.2172/6057803"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e2a80",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Anyway, the test statistic, appropriate if the original Kruskall-Wallis statistic ${\\color{royalblue}{T}}$ is significant at the $\\alpha$ level, is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bc1c0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\frac{\n",
    "  {\\color{royalblue}{R_i}}/n_i-{\\color{royalblue}{R_j}}/n_j\n",
    "}\n",
    "{\n",
    "  \\sqrt{\n",
    "    {\\color{royalblue}{S^2}}\\frac{N-1-{\\color{royalblue}{T}}}{N-k}\n",
    "    \\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)\n",
    "  }\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb05c35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which is to be compared to the $\\alpha/2$ tail of the Student-$t$ distribution with $N-k$ degrees of freedom.  Here\n",
    "$$\n",
    "{\\color{royalblue}{S^2}} = \\frac{1}{N-1}\\sum_{i=1}^k \\sum_{j=1}^{n_i}\n",
    "    \\left({\\color{royalblue}{R_{ij}}}-\\overline{R}\\right)^2\n",
    "$$\n",
    "is the normalizing quantity in the denominator of the Kruskall-Wallis statistic, which is equal to $\\frac{N(N+1)}{12}$ if there are no ties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399bc59",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Returning to our example, recall we got $p=0.125$ from the Kruskal-Wallis test, so there's not really any evidence of the samples differing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3c8d59b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12531520484413722"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chi2(df=k-1).sf(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b27e04",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But suppose the data had been a little different, and indicated a more significant mismatch (keeping the same sizes so we don't have to recompute $n_i$, $k$, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0be4a59f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([5., 2., 6., 1.]),\n",
       " array([3., 4., 8.]),\n",
       " array([10.,  9., 12.,  7., 11.])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newx_i_j = [\n",
    "    np.array([ 14.97,   5.80,  15.03,   5.50 ]),\n",
    "    np.array([  5.83,  13.96,  21.96 ]),\n",
    "    np.array([ 27.89,  23.03,  61.09,   18.62,  55.51 ])\n",
    "]\n",
    "newx_r = np.concatenate(newx_i_j)\n",
    "newR_r = stats.rankdata(newx_r)\n",
    "newR_i_j = [newR_r[i_r==i] for i in range(k)]; newR_i_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17e76f5f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14., 15., 49.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newR_i = np.array([np.sum(Ri_j) for Ri_j in newR_i_j]); newR_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a01a88b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.476923076923077"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newT = (N-1) * np.sum((newR_i-n_i*Rbar)**2/n_i) / np.sum((newR_r-Rbar)**2); newT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a36c1218",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023790676031139445"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chi2(df=k-1).sf(newT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0452f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now it's significant at below the 5% level, so we can ask which differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "374dfec8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.0, 13.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newSsq = np.sum((newR_r-Rbar)**2)/(N-1); newSsq, N*(N+1)/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52ac144a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.5, 5. , 9.8]), 6.5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newRbar_i = newR_i/n_i; newRbar_i, Rbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e97bb74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -0.62453835, -2.98648642],\n",
       "       [ 0.62453835,  0.        , -2.0901051 ],\n",
       "       [ 2.98648642,  2.0901051 ,  0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newT_ii = (newRbar_i[:,None]-newRbar_i[None,:])/np.sqrt(newSsq*(N-1-T)/(N-k)*(1/n_i[:,None]+1/n_i[None,:])); newT_ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da429f85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.        , 0.54777771, 0.01528782],\n",
       "        [0.54777771, 1.        , 0.06617206],\n",
       "        [0.01528782, 0.06617206, 1.        ]]),\n",
       " 0.023790676031139445)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*stats.t(df=N-k).sf(np.abs(newT_ii)), stats.chi2(df=k-1).sf(newT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd7141",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You will examine this test more carefully on the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995e852-ea06-4688-9bb7-e4c1cfcbedfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
