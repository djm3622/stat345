{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6f5841",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STAT 345: Nonparametric Statistics\n",
    "\n",
    "## Lesson 06.1: The Conover Squared-Ranks Test for Equal Variances\n",
    "\n",
    "**Reading: Conover Section 5.3**\n",
    "\n",
    "*Prof. John T. Whelan*\n",
    "\n",
    "Thursday 27 February 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a84e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These lecture slides are in a computational notebook.  You have access to them through http://vmware.rit.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c2df2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Flat HTML and slideshow versions are also in MyCourses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b23852",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The notebook can run Python commands (other notebooks can use R or Julia; \"Ju-Pyt-R\").  Think: computational data analysis, not \"coding\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a7897",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Standard commands to activate inline interface and import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe735fcf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f45f4d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0,5.0)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbde15e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- So far, we've considered rank-based tests of whether two or more samples could have been drawn from distributions w/the same location parameter (median or mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4088eeb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now consider tests for the scale parameter (variance or the like)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3daba47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Given two independent samples, $\\{x_i|i=1,\\ldots,n\\}$ & $\\{y_j|j=1,\\ldots,m\\}$, $H_0$ says the distributions equally spread out.  $H_1$ could be one- or two-sided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e64c76",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall: parametric $F$-test looks at ratio of sample variances\n",
    "$$  s_x^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i-{{\\overline{x}}})^2 \\qquad\\hbox{and}\\qquad\n",
    "    s_y^2 = \\frac{1}{m-1}\\sum_{j=1}^m (y_j-{{\\overline{y}}})^2\n",
    "$$\n",
    "where ${{\\overline{x}}}=\\frac{1}{n}\\sum_{i=1}^n x_i$ & ${{\\overline{y}}}=\\frac{1}{m}\\sum_{j=1}^m y_j$; statistic is $F = \\frac{s_x^2}{s_y^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334dc75b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For rank-based nonparametric test, look at ranks of $U_i^2 = (x_i-{{\\overline{x}}})^2$ and $V_j^2 = (y_j-{{\\overline{y}}})^2$ instead of just averaging those quantites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e96ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Take all of the $\\{U_i^2=(x_i-{{\\overline{x}}})^2\\}$ and $\\{V_j^2=(y_j-{{\\overline{y}}})^2\\}$, rank them in order, and note the ranks $\\{R(U_i^2)\\}$ and $\\{R(V_j^2)\\}$.\n",
    "- An equivalent but easier computation is to take the square roots\n",
    "$U_i = {\\left\\lvert x_i-{{\\overline{x}}}\\right\\rvert}$ & $V_j = {\\left\\lvert y_j-{{\\overline{y}}}\\right\\rvert}$\n",
    "and rank those instead. Since the mapping from $U_i\\rightarrow U_i^2$ is\n",
    "monotonic, it won’t change the order, and thus we’ll get the exact same\n",
    "set of ranks if we use $\\{R(U_i)\\}$ and $\\{R(V_j)\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc07d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the example we introduced previously for the Wilcoxon-Mann-Whitney rank sum test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae05e38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.004,  8.534, 34.536, 12.254,  8.744]),\n",
       " array([ 2.3,  5. , 10.7,  3.4]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i = np.array([8.56, 5.03, 48.1, 1.31, 4.82]); y_j = np.array([15.0, 12.3, 28.0, 13.9])\n",
    "n = len(x_i); m = len(y_j); N = n+m; xbar = np.mean(x_i); ybar = np.mean(y_j);\n",
    "U_i = np.abs(x_i - xbar); V_j = np.abs(y_j - ybar); U_i, V_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0155c442",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4., 5., 9., 8., 6., 1., 3., 7., 2.]),\n",
       " array([4., 5., 9., 8., 6., 1., 3., 7., 2.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UV_r = np.concatenate((U_i,V_j)); R_r = stats.rankdata(UV_r);\n",
    "R_r, stats.rankdata(np.concatenate(((x_i-xbar)**2,(y_j-ybar)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f13740a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4., 5., 9., 8., 6.]), array([1., 3., 7., 2.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RU_i = R_r[:n]; RV_j = R_r[n:]; RU_i, RV_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f72393",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The obvious thing would be to sum the ranks of the $\\{U_i\\}$, but it turns out you generally get a more powerful test if you square the ranks\n",
    "before adding them, so the test statistic is\n",
    "$$T_u = \\sum_{i=1}^{n} \\bigl(R(U_i)\\bigr)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31626c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Not obvious this is the best thing to do, but sort of analogous to sum of squares in parametric tests.  You'll investigate on the homework and see empirically that $\\sum_{i=1}^{n} \\bigl(R(U_i)\\bigr)^2$ tends\n",
    "to lead to a more powerful test statistic than $\\sum_{i=1}^{n} R(U_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd0ccd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that\n",
    "$$ T_u + T_v =\n",
    "\\sum_{i=1}^{n} \\bigl(R(U_i)\\bigr)^2 + \\sum_{j=1}^{m} \\bigl(R(V_j)\\bigr)^2\n",
    "=\\sum_{r=1}^N R_r^2 = N \\overline{R^2}\n",
    "$$\n",
    "is determined only by the ranks $\\{R_r\\}$ and not how they're partitioned between the $U$s and $V$s, so\n",
    "$T_u$ carries the same information as $T_v$, if you know $\\overline{R^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59c9d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If there are no ties, the ranks are just $\\{1,\\ldots,N\\}$ and something called Faulhaber's formula tells is $T_u+T_v=\\sum_{r=1}^N R_r^2 = N \\overline{R^2}=\\frac{N(N+1)(2N+1)}{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e0cba2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285.0, 285)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(R_r**2), N*(N+1)*(2*N+1)//6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bba965",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If there are ties, we assign the average ranks in the usual way, but while $\\sum_{r=1}^N R_r$ doesn't change if there are ties, $\\sum_{r=1}^N R_r^2$ does.  E.g., $2.5+2.5 = 5 = 2+3$ but\n",
    "$$(2.5)^2 + (2.5)^2 = 6.25 + 6.25 = 12.5 \\ne 2^2 + 3^2 = 4 + 9 = 13$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf104241",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that $E({\\color{royalblue}{T_u}})=n\\overline{R^2}$, $E({\\color{royalblue}{T_v}})=m\\overline{R^2}$, and\n",
    "$$\n",
    "T_u - n\\overline{R^2} = \\frac{(N-n)T_u-nT_v}{N} = \\frac{mT_u-nT_v}{N} = -(T_v - m\\overline{R^2})\n",
    "$$\n",
    "so $T_u - n\\overline{R^2}$ will have zero expectation value, and treat the two samples the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4077bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You might reasonably complain that $E({\\color{royalblue}{T_u}})$ should be a fixed number, but like the variance of other rank-based statistics, it depends on the observed ranks, specifically on how many ties there are and where. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13210f17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The “dirty secret” of methods like this is that this is actually a conditional expectation value $E({\\color{royalblue}{T_u}}|\\overline{R^2})$, conditional upon the full set of $N$ available ranks with the actual observed ties taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbfd3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- I.e.,\n",
    "our statements about expectations, significance, power, etc, all pertain\n",
    "to repeating the experiment but only considering repetitions that have\n",
    "this particular set of ranks including ties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82b97e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is kind of\n",
    "unsatisfying from a frequentist point of view, but it’s necessary to be\n",
    "able to describe the properties of the test in a distribution-free way,\n",
    "since you’d have to know the underlying distribution to know how likely\n",
    "ties are. Note that this is no big deal to a Bayesian, who is always\n",
    "constructing probabilities for unknown quantities conditional upon the\n",
    "data which were actually observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf153c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Given that $T_u - n\\overline{R^2}=-(T_v - m\\overline{R^2})$, not surprising that variance treats the samples symmetrically:\n",
    "$$\\operatorname{Var}(T_u - n\\overline{R^2})\n",
    "= \\frac{nm}{N-1}\\overline{\\bigl(R^2-\\overline{R^2}\\bigr)^2}\n",
    "= \\frac{nm}{N-1}\\left(\\overline{R^4}-\\bigl(\\overline{R^2}\\bigr)^2\\right)$$\n",
    "where $$\\overline{R^4}\n",
    "  = \\frac{1}{N}\n",
    "  \\left(\n",
    "    \\sum_{i=1}^n \\bigl(R(U_i)\\bigr)^4 + \\sum_{j=1}^m \\bigl(R(V_j)\\bigr)^4\n",
    "  \\right)$$ which is again equal to a constant\n",
    "($\\frac{1}{N}\\sum_{r=1}^N r^4=\\frac{(N+1)(2N+1)(3N^2+3N-1)}{30}$) if\n",
    "there are no ties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a4bc2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The statistic\n",
    "$$\n",
    "T_1 = \\frac{T_u - n\\overline{R^2}}{\n",
    "    \\sqrt{\n",
    "      \\frac{nm}{N-1}\\left(\\overline{R^4}-\\bigl(\\overline{R^2}\\bigr)^2\\right)\n",
    "    }\n",
    "  } = \\frac{T_u - n\\overline{R^2}}{\n",
    "    \\sqrt{\n",
    "      \\frac{nm}{N-1}\\overline{\\left(R^2-\\overline{R^2}\\right)^2}\n",
    "    }\n",
    "  }\n",
    "$$\n",
    "is approximately standard normal (under $H_0$) for large samples,\n",
    "and can be used to construct hypothesis tests, $p$-values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8acc9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16., 25., 81., 64., 36.,  1.,  9., 49.,  4.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsq_r = R_r**2; Rsq_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87becd4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since there are no ties in this example, $\\overline{R^2}=\\frac{(N+1)(2N+1)}{6}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc97bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.666666666666668, 31.666666666666668)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsqbar = np.mean(Rsq_r); Rsqbar, (N+1)*(2*N+1)/6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab9e609",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222.0, 63.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU = np.sum(RU_i**2); TV = np.sum(RV_j**2); TU, TV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3dd77e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As noted above, $T_u-m\\overline{R^2}=-\\left(T_v-n\\overline{R^2}\\right)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3591b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63.66666666666666, -63.66666666666667)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU-n*Rsqbar, TV-m*Rsqbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ca3706",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1752.222222222222"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varT = n*m/(N-1) * np.mean((Rsq_r-Rsqbar)**2); varT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851020a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We construct the statistic which should be approximately standard normal, and find it is at about the 93.6th percentile of the null distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e46e7a6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5209590473083399"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1 = (TU-n*Rsqbar)/np.sqrt(varT); T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8efbf5fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9358649426940304"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.norm.cdf(T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf07d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This would correspond to a two-sided $p$-value of about $0.13$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6614dd46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12827011461193927"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*stats.norm.sf(T1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2f141",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact Null Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fad444",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If there are no ties, we can use the exact distribution of the statistic under the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de7470",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Similar to Wilcoxon rank-sum case; $H_0$ says any combination of $n$ out of the $N$ available ranks $1,\\ldots,N$ can be the $R(U_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18900c88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Because we square the ranks before adding them, the null distribution is not the same as for the rank sum statistic.  Conover lists some percentiles of the null distribution in Table A9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79adfd4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It's not available in Python or R, but we can compute it by making a list of all the possible combinations and constructing the squared ranks statistic for each of them.  (We could have done this for the rank sum statistic too, but we didn't bother because it was available in the software.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53b0ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We use the function `itertools.combinations()` from the `itertools` library to loop over the possible combinations of ranks.  (Note we only need the combinations and not the permutations because each of the $n!$ permutations corresponding to a given combination gives the same value for the statistic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969dd74b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.combinations at 0x7fe49d0a4f90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "itertools.combinations(range(1,N+1),n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa174ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The function actually returns an \"iterator\" which is something you can use inside a loop or similar structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe258c1c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4, 5)\n",
      "(1, 2, 3, 4, 6)\n",
      "(1, 2, 3, 4, 7)\n",
      "(1, 2, 3, 4, 8)\n",
      "(1, 2, 3, 4, 9)\n",
      "(1, 2, 3, 5, 6)\n",
      "(1, 2, 3, 5, 7)\n",
      "(1, 2, 3, 5, 8)\n",
      "(1, 2, 3, 5, 9)\n",
      "(1, 2, 3, 6, 7)\n",
      "(1, 2, 3, 6, 8)\n",
      "(1, 2, 3, 6, 9)\n",
      "(1, 2, 3, 7, 8)\n",
      "(1, 2, 3, 7, 9)\n",
      "(1, 2, 3, 8, 9)\n",
      "(1, 2, 4, 5, 6)\n",
      "(1, 2, 4, 5, 7)\n",
      "(1, 2, 4, 5, 8)\n",
      "(1, 2, 4, 5, 9)\n",
      "(1, 2, 4, 6, 7)\n",
      "(1, 2, 4, 6, 8)\n",
      "(1, 2, 4, 6, 9)\n",
      "(1, 2, 4, 7, 8)\n",
      "(1, 2, 4, 7, 9)\n",
      "(1, 2, 4, 8, 9)\n",
      "(1, 2, 5, 6, 7)\n",
      "(1, 2, 5, 6, 8)\n",
      "(1, 2, 5, 6, 9)\n",
      "(1, 2, 5, 7, 8)\n",
      "(1, 2, 5, 7, 9)\n",
      "(1, 2, 5, 8, 9)\n",
      "(1, 2, 6, 7, 8)\n",
      "(1, 2, 6, 7, 9)\n",
      "(1, 2, 6, 8, 9)\n",
      "(1, 2, 7, 8, 9)\n",
      "(1, 3, 4, 5, 6)\n",
      "(1, 3, 4, 5, 7)\n",
      "(1, 3, 4, 5, 8)\n",
      "(1, 3, 4, 5, 9)\n",
      "(1, 3, 4, 6, 7)\n",
      "(1, 3, 4, 6, 8)\n",
      "(1, 3, 4, 6, 9)\n",
      "(1, 3, 4, 7, 8)\n",
      "(1, 3, 4, 7, 9)\n",
      "(1, 3, 4, 8, 9)\n",
      "(1, 3, 5, 6, 7)\n",
      "(1, 3, 5, 6, 8)\n",
      "(1, 3, 5, 6, 9)\n",
      "(1, 3, 5, 7, 8)\n",
      "(1, 3, 5, 7, 9)\n",
      "(1, 3, 5, 8, 9)\n",
      "(1, 3, 6, 7, 8)\n",
      "(1, 3, 6, 7, 9)\n",
      "(1, 3, 6, 8, 9)\n",
      "(1, 3, 7, 8, 9)\n",
      "(1, 4, 5, 6, 7)\n",
      "(1, 4, 5, 6, 8)\n",
      "(1, 4, 5, 6, 9)\n",
      "(1, 4, 5, 7, 8)\n",
      "(1, 4, 5, 7, 9)\n",
      "(1, 4, 5, 8, 9)\n",
      "(1, 4, 6, 7, 8)\n",
      "(1, 4, 6, 7, 9)\n",
      "(1, 4, 6, 8, 9)\n",
      "(1, 4, 7, 8, 9)\n",
      "(1, 5, 6, 7, 8)\n",
      "(1, 5, 6, 7, 9)\n",
      "(1, 5, 6, 8, 9)\n",
      "(1, 5, 7, 8, 9)\n",
      "(1, 6, 7, 8, 9)\n",
      "(2, 3, 4, 5, 6)\n",
      "(2, 3, 4, 5, 7)\n",
      "(2, 3, 4, 5, 8)\n",
      "(2, 3, 4, 5, 9)\n",
      "(2, 3, 4, 6, 7)\n",
      "(2, 3, 4, 6, 8)\n",
      "(2, 3, 4, 6, 9)\n",
      "(2, 3, 4, 7, 8)\n",
      "(2, 3, 4, 7, 9)\n",
      "(2, 3, 4, 8, 9)\n",
      "(2, 3, 5, 6, 7)\n",
      "(2, 3, 5, 6, 8)\n",
      "(2, 3, 5, 6, 9)\n",
      "(2, 3, 5, 7, 8)\n",
      "(2, 3, 5, 7, 9)\n",
      "(2, 3, 5, 8, 9)\n",
      "(2, 3, 6, 7, 8)\n",
      "(2, 3, 6, 7, 9)\n",
      "(2, 3, 6, 8, 9)\n",
      "(2, 3, 7, 8, 9)\n",
      "(2, 4, 5, 6, 7)\n",
      "(2, 4, 5, 6, 8)\n",
      "(2, 4, 5, 6, 9)\n",
      "(2, 4, 5, 7, 8)\n",
      "(2, 4, 5, 7, 9)\n",
      "(2, 4, 5, 8, 9)\n",
      "(2, 4, 6, 7, 8)\n",
      "(2, 4, 6, 7, 9)\n",
      "(2, 4, 6, 8, 9)\n",
      "(2, 4, 7, 8, 9)\n",
      "(2, 5, 6, 7, 8)\n",
      "(2, 5, 6, 7, 9)\n",
      "(2, 5, 6, 8, 9)\n",
      "(2, 5, 7, 8, 9)\n",
      "(2, 6, 7, 8, 9)\n",
      "(3, 4, 5, 6, 7)\n",
      "(3, 4, 5, 6, 8)\n",
      "(3, 4, 5, 6, 9)\n",
      "(3, 4, 5, 7, 8)\n",
      "(3, 4, 5, 7, 9)\n",
      "(3, 4, 5, 8, 9)\n",
      "(3, 4, 6, 7, 8)\n",
      "(3, 4, 6, 7, 9)\n",
      "(3, 4, 6, 8, 9)\n",
      "(3, 4, 7, 8, 9)\n",
      "(3, 5, 6, 7, 8)\n",
      "(3, 5, 6, 7, 9)\n",
      "(3, 5, 6, 8, 9)\n",
      "(3, 5, 7, 8, 9)\n",
      "(3, 6, 7, 8, 9)\n",
      "(4, 5, 6, 7, 8)\n",
      "(4, 5, 6, 7, 9)\n",
      "(4, 5, 6, 8, 9)\n",
      "(4, 5, 7, 8, 9)\n",
      "(4, 6, 7, 8, 9)\n",
      "(5, 6, 7, 8, 9)\n"
     ]
    }
   ],
   "source": [
    "for thisRU_i in itertools.combinations(range(1,N+1),n):\n",
    "    print(thisRU_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9947770",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You make an array of the values it loops over by using a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa95034e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RU_Ii = np.array([thisRU_i for thisRU_i in itertools.combinations(range(1,N+1),n)]); RU_Ii.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd88ca5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are $\\binom{N}{n}=\\frac{N!}{n!m!}$ equally likely combinations, which in this case is $126$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ea2529",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import comb as nchoosek\n",
    "nchoosek(N,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221a8ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So for each of these combinations we construct the statistic $T_u$ and get a list of equally likely values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d25b025",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 55,  66,  75,  79,  82,  87,  88,  90,  94,  95,  99, 100, 103,\n",
       "       103, 106, 110, 111, 111, 114, 114, 115, 115, 118, 120, 120, 121,\n",
       "       123, 126, 127, 127, 127, 129, 130, 130, 131, 132, 134, 135, 135,\n",
       "       135, 138, 138, 139, 142, 142, 143, 143, 144, 145, 146, 147, 148,\n",
       "       150, 151, 151, 152, 154, 155, 155, 156, 158, 159, 159, 159, 159,\n",
       "       160, 162, 162, 163, 165, 166, 166, 167, 168, 169, 171, 171, 172,\n",
       "       174, 174, 175, 175, 175, 176, 178, 179, 180, 180, 183, 183, 183,\n",
       "       186, 186, 187, 190, 190, 191, 191, 192, 194, 195, 195, 198, 199,\n",
       "       200, 201, 204, 206, 207, 207, 207, 210, 211, 214, 215, 219, 220,\n",
       "       222, 223, 228, 231, 234, 235, 239, 246, 255])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU_I = np.sort(np.sum(RU_Ii**2,axis=-1)); TU_I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634d9f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall the actual value of $T_u$ for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "effbb161",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57473cd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can calculate the cdf and $p$-value and see that the normal approximation ($p=0.13$) was not so far off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06eb304e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9365079365079365"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(TU_I <= TU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bc60304",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*np.mean(TU_I >= TU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c22b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can also see from the list that there are 9 values of the statistic equal to or greater than 222."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf5e238e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(TU_I >= TU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d31accb2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([222, 223, 228, 231, 234, 235, 239, 246, 255])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TU_I[TU_I >= TU]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20775253",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extension to $k$-Sample Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babcf1a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The squared ranks test can be extended to the case of $k$ independent samples, just as the Kruskal-Wallis test is defined as the $k$-sample generalization of the Wilcoxon rank-sum test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf58a19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The test statistic is written in terms of the squared ranks\n",
    "$S_i = \\sum_{j=1}^{n_i} \\bigl(R({\\lvert x_{ij}-{{\\overline{x}}}_i\\rvert})\\bigr)^2$\n",
    "as $$\\frac{\\sum_{i=1}^k \\frac{S_i^2}{n_i} - N\\left(\\overline{R^2}\\right)^2}\n",
    "  {\n",
    "    \\frac{N}{N-1}\n",
    "    \\left(\n",
    "      \\overline{R^4}-N\\bigl(\\overline{R^2}\\bigr)^2\n",
    "    \\right)\n",
    "  }=\\frac{\\sum_{i=1}^k \\frac{1}{n_i}\\bigl(S_i - n_i\\overline{R^2}\\bigr)^2}\n",
    "  {\n",
    "    \\frac{N}{N-1}\n",
    "    \\overline{\\bigl(R^2-\\overline{R^2}\\bigr)^2}\n",
    "  }$$\n",
    "where $\\overline{R^2}=\\frac{1}{N}\\sum_{r=1}^N R_r^2$ is also called $\\overline{S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bf99a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The null distribution is approximately $\\chi^2(k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb10273",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Look at the example from last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d0ace2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 3, 5]), 3, 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i_j = [np.array([ 14.97,   5.80,  25.03,   5.50 ]),\n",
    "       np.array([  5.83,  13.96,  21.96]),\n",
    "       np.array([ 17.89,  23.03,  61.09,   18.62,  55.51])]\n",
    "n_i = np.array([len(xi_j) for xi_j in x_i_j]); k = len(n_i); N = np.sum(n_i); n_i,k,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c9d5dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.825     , 13.91666667, 35.228     ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbar_i = np.array([np.mean(xi_j) for xi_j in x_i_j]); xbar_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da53b080",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.145,  7.025, 12.205,  7.325]),\n",
       " array([8.08666667, 0.04333333, 8.04333333]),\n",
       " array([17.338, 12.198, 25.862, 16.608, 20.282])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_i_j = [np.abs(xi_j-np.mean(xi_j)) for xi_j in x_i_j]; U_i_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92dd9daf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2., 3., 8., 4.]),\n",
       " array([6., 1., 5.]),\n",
       " array([10.,  7., 12.,  9., 11.])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_r = np.concatenate(U_i_j); RU_r = stats.rankdata(U_r)\n",
    "i_r = np.concatenate([(i,)*n_i[i] for i in range(k)])\n",
    "RU_i_j = [RU_r[i_r==i] for i in range(k)]; RU_i_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1580db0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now construct the sums of the squared ranks $S_i = \\sum_{j=1}^{n_i} \\bigl(R({\\lvert x_{ij}-{{\\overline{x}}}_i\\rvert})\\bigr)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a195c011",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 93.,  62., 495.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_i = np.array([np.sum(RUi_j**2) for RUi_j in RU_i_j]); S_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12e7767",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The average squared rank $\\overline{S}=\\frac{1}{N}\\sum_{r=1}^N R_r^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bf84e61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.166666666666664"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sbar = np.mean(RU_r**2); Sbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34e2ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The numerator $\\sum_{i=1}^k \\frac{S_i^2}{n_i} - N\\left(\\overline{R^2}\\right)^2=\\sum_{i=1}^k \\frac{1}{n_i}\\bigl(S_i - n_i\\overline{R^2}\\bigr)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d17fe17d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17240.250000000007, 17240.25)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(S_i**2/n_i)-N*Sbar**2, np.sum((S_i-n_i*Sbar)**2/n_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fac105",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The denominator $D^2=\n",
    "    \\frac{N}{N-1}\n",
    "    \\overline{\\bigl(R^2-\\overline{R^2}\\bigr)^2}=    \\frac{N}{N-1}\n",
    "\\left(\n",
    "      \\overline{R^4}-\\bigl(\\overline{R^2}\\bigr)^2\n",
    "    \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "044df1d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2318.3333333333335, 2318.333333333334, 2318.333333333334)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dsq=N/(N-1)*np.mean((RU_r**2-Sbar)**2); Dsq, N/(N-1)*(np.mean(RU_r**4)-Sbar**2),(np.sum(RU_r**4)-N*Sbar**2)/(N-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259f9f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tip: it's easy to mess up the cancellation in the \"shortcut formula\" versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c33422f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-32889.99999999999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(RU_r**4-N*Sbar**2)/(N-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916334d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Obviously wrong since $D^2>0$ by construction.  Can avoid this by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b31da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sanity-checking the values of intermediate quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b6a12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using versions which square differences, rather than subtracting squares (also avoids roundoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f0670",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now assemble the statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f64eb39f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.436484543493889"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.sum((S_i-n_i*Sbar)**2/n_i)/Dsq; T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d280605",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, get the $p$-value from the upper tail (only!) of the $\\chi^2(k-1)$ distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13a6861e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024276602034339175"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.chi2(df=k-1).sf(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809482a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More tips on the multinomial iterator from problem set 05.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dba669",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problem 5.2.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebab685",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Find the exact distribution of the Kruskal-Wallis test statistic when\n",
    "$H_0$ is true, $n_1=3$, $n_2=2$, $n_3=1$, and there are no ties. Compare\n",
    "your results with the quantiles given in Table A8.\n",
    "\\[Also compare with the chi-squared approximation.\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434ad26",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I pointed to a function called `multinomial_combinations`, which is now part of the `combinatorics` library available from https://phillipmfeldman.org/Python/combinatorics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "736d06b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from combinatorics import m_way_ordered_combinations as multinomial_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3b8a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is an \"iterator\", which lets you loop through the $\\frac{6!}{3!2!1!}=60$ different ways of partitioning the $N=6$ ranks $\\{1,2,3,4,5,6\\}$ into groups of $n_1=3$, $n_2=2$, and $n_3=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7618b57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_i = np.array([3,2,1]); N = np.sum(n_i); k = len(n_i); R_r = np.arange(1,N+1); R_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda03a43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each time I access the iterator, it gives me a new example combination of the ranks $R_r$ into $\\{R_{1j}\\}$, $\\{R_{2j}\\}$, and $\\{R_{3j}\\}$, so I could use it to construct $R_1=\\sum_{j=1}^{n_1}R_{1j}$, $R_2=\\sum_{j=1}^{n_2}R_{2j}$, and $R_3=\\sum_{j=1}^{n_3}R_{3j}$ corresponding to that combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd5932a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 2, 3), (4, 5), (6,))\n",
      "6 9 6\n",
      "((1, 2, 3), (4, 6), (5,))\n",
      "6 10 5\n",
      "((1, 2, 3), (5, 6), (4,))\n",
      "6 11 4\n",
      "((1, 2, 4), (3, 5), (6,))\n",
      "7 8 6\n",
      "((1, 2, 4), (3, 6), (5,))\n",
      "7 9 5\n",
      "((1, 2, 4), (5, 6), (3,))\n",
      "7 11 3\n",
      "((1, 2, 5), (3, 4), (6,))\n",
      "8 7 6\n",
      "((1, 2, 5), (3, 6), (4,))\n",
      "8 9 4\n",
      "((1, 2, 5), (4, 6), (3,))\n",
      "8 10 3\n",
      "((1, 2, 6), (3, 4), (5,))\n",
      "9 7 5\n",
      "((1, 2, 6), (3, 5), (4,))\n",
      "9 8 4\n",
      "((1, 2, 6), (4, 5), (3,))\n",
      "9 9 3\n",
      "((1, 3, 4), (2, 5), (6,))\n",
      "8 7 6\n",
      "((1, 3, 4), (2, 6), (5,))\n",
      "8 8 5\n",
      "((1, 3, 4), (5, 6), (2,))\n",
      "8 11 2\n",
      "((1, 3, 5), (2, 4), (6,))\n",
      "9 6 6\n",
      "((1, 3, 5), (2, 6), (4,))\n",
      "9 8 4\n",
      "((1, 3, 5), (4, 6), (2,))\n",
      "9 10 2\n",
      "((1, 3, 6), (2, 4), (5,))\n",
      "10 6 5\n",
      "((1, 3, 6), (2, 5), (4,))\n",
      "10 7 4\n",
      "((1, 3, 6), (4, 5), (2,))\n",
      "10 9 2\n",
      "((1, 4, 5), (2, 3), (6,))\n",
      "10 5 6\n",
      "((1, 4, 5), (2, 6), (3,))\n",
      "10 8 3\n",
      "((1, 4, 5), (3, 6), (2,))\n",
      "10 9 2\n",
      "((1, 4, 6), (2, 3), (5,))\n",
      "11 5 5\n",
      "((1, 4, 6), (2, 5), (3,))\n",
      "11 7 3\n",
      "((1, 4, 6), (3, 5), (2,))\n",
      "11 8 2\n",
      "((1, 5, 6), (2, 3), (4,))\n",
      "12 5 4\n",
      "((1, 5, 6), (2, 4), (3,))\n",
      "12 6 3\n",
      "((1, 5, 6), (3, 4), (2,))\n",
      "12 7 2\n",
      "((2, 3, 4), (1, 5), (6,))\n",
      "9 6 6\n",
      "((2, 3, 4), (1, 6), (5,))\n",
      "9 7 5\n",
      "((2, 3, 4), (5, 6), (1,))\n",
      "9 11 1\n",
      "((2, 3, 5), (1, 4), (6,))\n",
      "10 5 6\n",
      "((2, 3, 5), (1, 6), (4,))\n",
      "10 7 4\n",
      "((2, 3, 5), (4, 6), (1,))\n",
      "10 10 1\n",
      "((2, 3, 6), (1, 4), (5,))\n",
      "11 5 5\n",
      "((2, 3, 6), (1, 5), (4,))\n",
      "11 6 4\n",
      "((2, 3, 6), (4, 5), (1,))\n",
      "11 9 1\n",
      "((2, 4, 5), (1, 3), (6,))\n",
      "11 4 6\n",
      "((2, 4, 5), (1, 6), (3,))\n",
      "11 7 3\n",
      "((2, 4, 5), (3, 6), (1,))\n",
      "11 9 1\n",
      "((2, 4, 6), (1, 3), (5,))\n",
      "12 4 5\n",
      "((2, 4, 6), (1, 5), (3,))\n",
      "12 6 3\n",
      "((2, 4, 6), (3, 5), (1,))\n",
      "12 8 1\n",
      "((2, 5, 6), (1, 3), (4,))\n",
      "13 4 4\n",
      "((2, 5, 6), (1, 4), (3,))\n",
      "13 5 3\n",
      "((2, 5, 6), (3, 4), (1,))\n",
      "13 7 1\n",
      "((3, 4, 5), (1, 2), (6,))\n",
      "12 3 6\n",
      "((3, 4, 5), (1, 6), (2,))\n",
      "12 7 2\n",
      "((3, 4, 5), (2, 6), (1,))\n",
      "12 8 1\n",
      "((3, 4, 6), (1, 2), (5,))\n",
      "13 3 5\n",
      "((3, 4, 6), (1, 5), (2,))\n",
      "13 6 2\n",
      "((3, 4, 6), (2, 5), (1,))\n",
      "13 7 1\n",
      "((3, 5, 6), (1, 2), (4,))\n",
      "14 3 4\n",
      "((3, 5, 6), (1, 4), (2,))\n",
      "14 5 2\n",
      "((3, 5, 6), (2, 4), (1,))\n",
      "14 6 1\n",
      "((4, 5, 6), (1, 2), (3,))\n",
      "15 3 3\n",
      "((4, 5, 6), (1, 3), (2,))\n",
      "15 4 2\n",
      "((4, 5, 6), (2, 3), (1,))\n",
      "15 5 1\n"
     ]
    }
   ],
   "source": [
    "# Note the new library complains if we pass it n_i as a NumPy array, so we convert it to a list\n",
    "for R_i_j in multinomial_combinations(R_r,list(n_i)):\n",
    "    print(R_i_j)\n",
    "    R1 = np.sum(R_i_j[0])\n",
    "    R2 = np.sum(R_i_j[1])\n",
    "    R3 = np.sum(R_i_j[2])\n",
    "    print(R1,R2,R3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa58a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can either do a loop like the one above, and compute another equally likely value of the Kruskal-Wallis statistic for each one, or you can combine them all in a list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32189f8d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "R_I_i_j = [R_i_j for R_i_j in multinomial_combinations(R_r,list(n_i))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58f6f2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are clever ways to get the array of rank-sums $R^{(I)}_i$ from this, but a little more brute-force is just to pull out the $R^{(I)}_{1j}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f27a683",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1_Ij = np.array([R_i_j[0] for R_i_j in R_I_i_j]); R1_Ij.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213584a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and then compute $R^{(I)}_1=\\sum_{j=1}^{n_1}R^{(I)}_{1j}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9643b3f8",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  6,  6,  7,  7,  7,  8,  8,  8,  9,  9,  9,  8,  8,  8,  9,  9,\n",
       "        9, 10, 10, 10, 10, 10, 10, 11, 11, 11, 12, 12, 12,  9,  9,  9, 10,\n",
       "       10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12, 13, 13, 13, 12, 12, 12,\n",
       "       13, 13, 13, 14, 14, 14, 15, 15, 15])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1_I = R1_Ij.sum(axis=-1); R1_I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a9146",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can do likewise for $R^{(I)}_2=\\sum_{j=1}^{n_2}R^{(I)}_{2j}$ and $R^{(I)}_3=\\sum_{j=1}^{n_3}R^{(I)}_{3j}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1477dcc5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_Ij = np.array([R_i_j[1] for R_i_j in R_I_i_j]); R2_Ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ef804e1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 10, 11,  8,  9, 11,  7,  9, 10,  7,  8,  9,  7,  8, 11,  6,  8,\n",
       "       10,  6,  7,  9,  5,  8,  9,  5,  7,  8,  5,  6,  7,  6,  7, 11,  5,\n",
       "        7, 10,  5,  6,  9,  4,  7,  9,  4,  6,  8,  4,  5,  7,  3,  7,  8,\n",
       "        3,  6,  7,  3,  5,  6,  3,  4,  5])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_I = R2_Ij.sum(axis=-1); R2_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e263d988",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R3_Ij = np.array([R_i_j[2] for R_i_j in R_I_i_j]); R3_Ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29e8eb34",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 4, 6, 5, 3, 6, 4, 3, 5, 4, 3, 6, 5, 2, 6, 4, 2, 5, 4, 2, 6,\n",
       "       3, 2, 5, 3, 2, 4, 3, 2, 6, 5, 1, 6, 4, 1, 5, 4, 1, 6, 3, 1, 5, 3,\n",
       "       1, 4, 3, 1, 6, 2, 1, 5, 2, 1, 4, 2, 1, 3, 2, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R3_I = R3_Ij.sum(axis=-1); R3_I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd49d2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can manipulate these, and check, for example, that $R_1+R_2+R_2=\\frac{N(N+1)}{2}=21$ for each of the possible combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81de8b75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1_I + R2_I + R3_I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc44376",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given these three arrays of $R^{(I)}_1$, $R^{(I)}_2$, and $R^{(I)}_3$, we can construct the 60 equally-likely Kruskal-Wallis statistics $T^{(I)}$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
