{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ebbab5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STAT 345: Nonparametric Statistics\n",
    "\n",
    "## Lesson 05.1: Measures of Power and Efficiency\n",
    "\n",
    "**Reading: Conover Section 2.4**\n",
    "\n",
    "*Prof. John T. Whelan*\n",
    "\n",
    "Tuesday 18 February 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467a7c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These lecture slides are in a computational notebook.  You have access to them through http://vmware.rit.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef40bbc6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Flat HTML and slideshow versions are also in MyCourses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0e386",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The notebook can run Python commands (other notebooks can use R or Julia; \"Ju-Pyt-R\").  Think: computational data analysis, not \"coding\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2489e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Standard commands to activate inline interface and import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04288020",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc8807",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0,5.0)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef4998",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- General framework of hypothesis testing: define statistic $T({{\\mathbf{x}}})$ from observed data ${{\\mathbf{x}}}=\\{x_i|i=1,\\ldots,n\\}$; if $T({{\\mathbf{x}}})>c$, we reject null hypothesis $H_0$.  (Also for lower-tailed or two-sided tests if you define the statistic as $-z$ or $\\lvert z\\rvert$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d56293",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Significance $\\alpha = P(T({{\\mathbf{{\\color{royalblue}{X}}}}}){\\mathbin{>}}c|H_0)$ is prob of type I error (false alarm) if null $H_0$ is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735b5bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Power $\\gamma(\\theta) = P(T({{\\mathbf{{\\color{royalblue}{X}}}}}){\\mathbin{>}}c|H_1(\\theta))$ is prob of detection (efficiency) if alternative hypothesis $H_1(\\theta)$ (possibly w/effect size $\\theta$) is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f9ff4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ways to evaluate or compare tests (choice of statistic $T({{\\mathbf{x}}})$):<br>\n",
    "$\\quad\\ \\ \\bullet\\ $Power curve: hold threshold $c$ \\& sample size $n$ fixed (so $\\alpha$ fixed) & plot $\\gamma(\\theta)$ vs $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772214c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * ROC curve: hold $n$ \\& $\\theta$ fixed; vary threshold $c$ & plot power $\\gamma$ vs significance $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeaa656",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * Asymptotic relative efficiency: hold $c_1$, $c_2$ \\& $\\theta$ fixed; find ratio of sample sizes $n_2/n_1$ so that $T_1(\\mathbf{x})>c_1$ w/sample of size $n_1$ & $T_2(\\mathbf{x})>c_2$ w/sample of size $n_2$ have the same $\\alpha$ & $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fbdf6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For concreteness, consider two examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a82fcd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   $t$-test, where $H_0$ says sampling dist has mean $\\mu=0$ vs 1-sided $H_1$: $\\mu>0$. Test statistic is\n",
    "    $$T_t({{\\mathbf{x}}}) = \\frac{{{\\overline{x}}}}{\\sqrt{s^2/n}}$$\n",
    "    where $n$ is size of the sample ${{\\mathbf{x}}}\\equiv\\{x_i|i=1,\\ldots,n\\}$,\n",
    "    ${{\\overline{x}}}=\\frac{1}{n}\\sum_{i=1}^n x_i$ is the sample mean,\n",
    "    and $s^2=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-{{\\overline{x}}})^2$ is the\n",
    "    sample variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709b465",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   One-sample sign test, where $H_0$ says sampling dist median $x_{0.5}=0$ vs $H_1$: $x_{0.5}>0$. Test statistic is the number of positive values\n",
    "    $$T_s({{\\mathbf{x}}}) = \\#_i(x_i{\\mathbin{>}}0) = \\sum_{i=1}^n I_{x{\\mathbin{>}}0}(x_i)$$\n",
    "    where $I_{x{\\mathbin{>}}0}(x)$ is the **indicator function** which is $1$ if $x>0$ and $0$ if $x<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0317b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For robust inference, both $H_0$ and $H_1$ are composite hypotheses.\n",
    "- $H_0$ specifies mean or median of sampling distribution, but not its form. Is it normal, or Student-$t$, or Laplace (double exponential)? (For all of these, the mean and median are the same.) Does it violate assumptions of one\n",
    "of the tests, like the Cauchy distribution (which has a median but whose mean is undefined), or are its mean and\n",
    "median different, like a gamma distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06634d4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The alternative hypotheses ($H_1$) also don't specify distribution, but also don’t specify value of the non-zero location parameter $\\theta$.  We call this the size of the effect, a\n",
    "measure of the amount by which $H_1$ differs from $H_0$. Assume\n",
    "for simplicity that $\\theta=0$ corresponds to null hypothesis $H_0$ and\n",
    "that $H_1$ specifies $\\theta>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee3b886",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Also see that the definitions of $T({{\\mathbf{x}}})$ above depend on the size $n$ of the data sample (a property of $\\mathbf{x}$ which we know before we’ve collected the data).  Threshold $c$ of interest will be set in a way that depends on $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f12bdd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, taking all this into account, the significance $\\alpha$ of a test\n",
    "depends on:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81890653",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   The choice of test statistic $T({{\\mathbf{x}}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f6cbc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   The form of null sampling distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec950d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   The choice of threshold $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3473a04",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   The sample size $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09501226",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The power $\\gamma$ depends on all of that plus the effect size $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5b252c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can consider a comparison between two families of tests with\n",
    "statistics $T_1({{\\mathbf{x}}})$ and $T_2({{\\mathbf{x}}})$. The\n",
    "comparison is to be done under the assumption of a null sampling\n",
    "distribution, so there will be one comparison assuming a normal sampling\n",
    "distribution, one for Laplace, one for Student-$t$, etc. We can write the\n",
    "significance and power of each test as\n",
    "$$\\begin{gathered}\n",
    "    \\alpha_1 = \\alpha_1(c_1,n_1)\n",
    "    \\qquad\\hbox{and}\\qquad\n",
    "    \\gamma_1 = \\gamma_1(c_1,n_1,\\theta)\n",
    "    \\\\\n",
    "    \\alpha_2 = \\alpha_2(c_2,n_2)\n",
    "    \\qquad\\hbox{and}\\qquad\n",
    "    \\gamma_2 = \\gamma_2(c_2,n_2,\\theta)\n",
    "  \\end{gathered}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde8ca1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general, if $\\alpha_1=\\alpha_2$, $n_1=n_2$ and $\\gamma_1>\\gamma_2$,\n",
    "we say that $T_1$ gives us a more powerful (or efficient) test than\n",
    "$T_2$. But this may depend on the form of the sampling distribution and\n",
    "the significance $\\alpha$ and sample size $n$ at which the comparison is\n",
    "done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28048086",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that it doesn’t really make sense to compare the thresholds $c_1$\n",
    "and $c_2$, since they’re defined for different statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c947fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a7db0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One comparison: fix a value of\n",
    "$n=n_1=n_2$; choose $c_1$ and $c_2$ to get $\\alpha=\\alpha_1=\\alpha_2$ and then plot $\\gamma_1(\\theta)$ and\n",
    "$\\gamma_2(\\theta)$ as functions of the effect size $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df40cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "E.g., find the exact $\\alpha$ for a sign test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c6003",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n=80; nulldist = stats.binom(n,0.5); ycrit = nulldist.isf(0.05); realalpha = nulldist.sf(ycrit)\n",
    "print('The significance of a sign test with n=%d which rejects if n+ > %d is %f' % (n,ycrit,realalpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d650beb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To get the power, we have to know the actual sampling distribution under $H_1$; if it's $N(\\theta,1)$, $p_\\theta=P(\\color{royalblue}{X}>0|\\theta)=1-\\Phi(-\\theta)$, and $\\gamma_s(\\theta)=P(\\color{royalblue}{N_+}\\mathbin{>}47)$ comes from tail of $\\operatorname{Bin}(n,p_\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ed44d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta_t = np.linspace(0,0.6,101); sampledist_t = stats.norm(loc=theta_t); p theta_t = sampledist_t.sf(0.)\n",
    "altdist_t = stats.binom(n,ptheta_t); gammas_t = altdist_t.sf(ycrit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8e0dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On the other hand, if we use the central limit theorem (to avoid Monte Carlo), $E(\\color{royalblue}{X})=\\theta$ & $\\operatorname{Var}(\\color{royalblue}{X})=1$ mean the power curve for the $t$-test is\n",
    "$$\n",
    "\\gamma_t(\\theta)=1-\\Phi\\left(z_{1-\\alpha}-\\theta\\sqrt{n}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26368ef7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "realzcrit = stats.norm.isf(realalpha); realzcrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eeb58b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gammat_t = stats.norm(loc=(theta_t*np.sqrt(n))).sf(realzcrit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2a279",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To compare the two tests, we plot $\\gamma_s(\\theta)$ & $\\gamma_t(\\theta)$ vs $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986da07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_t,gammas_t,ls='-',label='sign test'); plt.plot(theta_t,gammat_t,ls='--',label=r'$t$-test');\n",
    "plt.legend(); plt.xlabel(r'$\\theta$'); plt.ylabel(r'$\\gamma(\\theta)$');\n",
    "plt.grid(); plt.xlim(0,0.6); plt.ylim(0,1); plt.title('Power curves for normal sampling distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe02965",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the sampling distribution is Laplace, but still with $E(\\color{royalblue}{X})=\\theta$ & $\\operatorname{Var}(\\color{royalblue}{X})=1$, the central limit theorem says $\\gamma_t(\\theta)=1-\\Phi\\left(z_{1-\\alpha}-\\theta\\sqrt{n}\\right)$ as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee9e0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stats.laplace(scale=np.sqrt(0.5)).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f279ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But for the power of the sign test, we need $p_\\theta=P(\\color{royalblue}{X}>0|\\theta)$ when $\\color{royalblue}{X}$ is Laplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517982b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lapdist_t = stats.laplace(loc=theta_t,scale=np.sqrt(0.5)); pthetalap_t = lapdist_t.sf(0.)\n",
    "altdistlap_t = stats.binom(n,pthetalap_t); gammaslap_t = altdistlap_t.sf(ycrit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04adde3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_t,ptheta_t,ls='-',label='Normal'); plt.plot(theta_t,pthetalap_t,ls='--',label=r'Laplace');\n",
    "plt.legend(); plt.xlabel(r'$\\theta$'); plt.ylabel(r'$p_\\theta$');\n",
    "plt.grid(); plt.xlim(0,0.6); plt.ylim(0.5,1); plt.title(r'$P(X>0)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1298c81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sanity check: for positive location parameter & same variance, more of Laplace distribution's area at positive $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bf237",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we compare the power curves w/same $n$ & $\\alpha$ as before, but now Laplace sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394a41e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(theta_t,gammaslap_t,ls='-',label='sign test'); plt.plot(theta_t,gammat_t,ls='--',label=r'$t$-test');\n",
    "plt.legend(); plt.xlabel(r'$\\theta$'); plt.ylabel(r'$\\gamma(\\theta)$');\n",
    "plt.grid(); plt.xlim(0,0.6); plt.ylim(0,1); plt.title('Power curves for Laplace sampling distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d02d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd620567",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To compare power curves, need to set $\\alpha_1=\\alpha_2=\\alpha$, which fixes thresholds $c_1$ & $c_2$,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df49e92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead consider families of tests which can be \"tuned\" changing threshold.  Lowering $c$ increases power $\\gamma$ but increases false alarm prob $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68977d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fix $n$ & $\\theta$ and consider how $\\alpha(c)$ & $\\gamma(c)$ change with $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b6506",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To compare tests, can't talk about \"the same threshold\" so don't plot $\\gamma_1(c_1)$ and $\\gamma_2(c_2)$ vs \"$c$\".  Instead consider $\\gamma_1(c_1)$ and $\\alpha_1(c_1)$ as functions which defining a parametrized curve in the $\\alpha$-$\\gamma$ plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec5ee4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\gamma$ vs $\\alpha$ is called an ROC (“Receiver Operating Characteristic”) curve, developed for radar during WWII.  More often seen in signal processing than classical stats, but a useful complement to other measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9a4c7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, let's construct the ROC for the sign test.  If the sample size is $n$, we can think of the sign test as a family of $n+2$ tests, which reject the null hypothesis if $t^{+}> c$ for $c\\in\\{-1,0,1,\\ldots,n\\}$.  (If $c=-1$, the test always rejects $H_0$, and $\\alpha=\\gamma=1$, while if $c=n$, the test never rejects $H_0$, and $\\alpha=\\gamma=0$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79741889",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 100; theta = 0.2; cs_c = np.arange(-1,n+1); cs_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3abe24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each value of $c$, we can calculate the probability of rejecting $H_0$ if the sample is drawn from the null dustribution, which is $\\alpha_s(c)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88535167",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p0 = 0.5; alphas_c = stats.binom(n,p0).sf(cs_c); alphas_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb09ff56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And likewise the probability of rejecting $H_0$ if the sample is drawn from $N(\\theta,1)$ for the chosen value $\\theta=0.2$ of the effect size, which is $\\gamma(c)$ or more completely $\\gamma_s(c;\\theta)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eaeaed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p1 = stats.norm(loc=theta).sf(0); p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb30ea22",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gammas_c = stats.binom(n,p1).sf(cs_c); gammas_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b68dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We make the ROC as a parametric plot of $\\gamma(c)$ vs $\\alpha(c)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9292a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6)); plt.plot(alphas_c,gammas_c,'b*',label='sign test');plt.plot(alphas_c,alphas_c,'k:');\n",
    "plt.xlabel(r'$\\alpha$');plt.ylabel(r'$\\gamma$');plt.title(r'ROC curve for $N(%.1f,1)$, $n=%d$' % (theta,n));\n",
    "plt.grid();plt.legend(loc='lower right');plt.xlim(0,1);plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0d49b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What if we want to find out the threshold which gives $\\alpha\\approx 0.2$, $\\gamma\\approx 0.8$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef1d4b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "flag02_c = (0.15 < alphas_c) & (alphas_c < 0.2); cs_c[flag02_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889300a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alphas_c[flag02_c], gammas_c[flag02_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75edfd2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gammas_c[flag02_c] - alphas_c[flag02_c], max(gammas_c-alphas_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0882a00a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not necessarily useful to maximize $\\gamma-\\alpha$ since often $\\alpha$ is proscribed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5cd43e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6)); plt.plot(alphas_c,gammas_c,'*',label='sign test');plt.plot(alphas_c,alphas_c,'k:');\n",
    "plt.xlabel(r'$\\alpha$');plt.ylabel(r'$\\gamma$');plt.title(r'ROC curve for $N(%.1f,1)$, $n=%d$' % (theta,n));\n",
    "plt.grid();plt.legend(loc='lower right');plt.xlim(0,1);plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2c7f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Convenient to draw line $\\gamma=\\alpha$, since the ROC curve for any\n",
    "unbiased ($\\gamma\\ge\\alpha$) test must lie above and to the left of that line. Also\n",
    "note a more powerful test will be above & to left of a\n",
    "less powerful one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f59cf1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For comparison, draw ROC curve for $t$-test w/$n=100$, $H_1$: $\\color{royalblue}{X}\\sim N(0.2,1)$.  Could use CLT again, but this time use Monte Carlo to be ready for situations w/o theoretical power curve.  Construct $N=10,\\!000$ samples of size $n=100$, $\\{x^{(I)}_i\\}$, from the null distribution, and construct the test statistic\n",
    "$$\n",
    "t^{(I)}_0=\\frac{\\overline{x}^{(I)}}{s^{(I)}\\left/\\sqrt{n}\\right.}\n",
    "$$\n",
    "in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc686670",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N = 10**4; np.random.seed(20250218); x_Ii = stats.norm.rvs(size=(N,n))\n",
    "xbar_I = np.mean(x_Ii,axis=-1); s_I = np.std(x_Ii,ddof=1,axis=-1)\n",
    "t0_I = xbar_I / (s_I/np.sqrt(n)); t1_I = (xbar_I+theta) / (s_I/np.sqrt(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0eb63e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Also use the usual trick of re-using the Monte Carlo samples and constructing the $t$-statistic you'd expect from $N(\\theta,1)$ as\n",
    "$$\n",
    "t^{(I)}_1=\\frac{\\overline{x}^{(I)}+\\theta}{s^{(I)}\\left/\\sqrt{n}\\right.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8945f72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In principle the $t$-test is a family of infinitely many tests which reject $H_0$ if $t\\ge c$ where $c$ can be any real number.  For the purpose of plotting the curve, we'll choose 1000 evenly-spaced points in $c\\in[-3,3+\\theta\\sqrt{n}]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28192d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cmin = -3; cmax = 3 + theta * np.sqrt(n); ct_c = np.linspace(cmin,cmax,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e83529",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our estimate of $\\alpha(c)$ is then the fraction of Monte Carlo samples in which $t^{(I)}_0\\ge c$, while $\\gamma(c)$ is the fraction in which $t^{(I)}_1\\ge c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cea388",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alphat_c = np.mean(t0_I[None,:] >= ct_c[:,None], axis=-1)\n",
    "gammat_c = np.mean(t1_I[None,:] >= ct_c[:,None], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d43ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6)); plt.plot(alphas_c,gammas_c,'-*',label='sign test');\n",
    "plt.plot(alphas_c,alphas_c,'k:');plt.plot(alphat_c,gammat_c,'--',label=r'$t$-test');\n",
    "plt.xlabel(r'$\\alpha$');plt.ylabel(r'$\\gamma$');plt.title(r'ROC curve for $N(%.1f,1)$, $n=%d$' % (theta,n));\n",
    "plt.grid();plt.legend(loc='lower right');plt.xlim(0,1);plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac21db0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a normal sampling distribution the $t$-test has a\n",
    "higher power $\\gamma$ at any given significance $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60988de1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we re-do\n",
    "the construction for the Laplace distribution, the situation is reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfd5ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p1=stats.laplace(loc=theta,scale=np.sqrt(0.5)).sf(0);alphas_c=stats.binom(n,p0).sf(cs_c);gammas_c=stats.binom(n,p1).sf(cs_c);\n",
    "np.random.seed(20230220); x_Ii = stats.laplace(scale=np.sqrt(0.5)).rvs(size=(N,n));xbar_I = np.mean(x_Ii,axis=-1);\n",
    "s_I = np.std(x_Ii,ddof=1,axis=-1);t0_I=xbar_I/(s_I/np.sqrt(n));t1_I=(xbar_I+theta)/(s_I/np.sqrt(n))\n",
    "plt.figure(figsize=(6,6));plt.plot(alphas_c,gammas_c,'-*',label='sign test');plt.plot(alphas_c,alphas_c,'k:'); \n",
    "plt.plot(alphat_c,gammat_c,'--',label=r'$t$-test');plt.xlabel(r'$\\alpha$'); plt.ylabel(r'$\\gamma$');\n",
    "plt.legend(loc='lower right');plt.xlim(0,1);plt.ylim(0,1);plt.grid(True);plt.title(r'ROC curve for Laplace w/mean %.1f, std 1, $n=%d$'% (theta,n));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06ffc8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since hypothesis tests get more interesting the lower the false alarm\n",
    "probability $\\alpha$, it’s sometimes helpful to do the ROC plot with\n",
    "$\\alpha$ on a log scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b27c98",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogx(alphas_c,gammas_c,'-*',label='sign test'); plt.semilogx(alphas_c,alphas_c,'k:');\n",
    "plt.semilogx(alphat_c,gammat_c,'--',label=r'$t$-test');plt.xlabel(r'$\\alpha$');plt.ylabel(r'$\\gamma$');\n",
    "plt.title(r'ROC curve for Laplace w/mean %.1f, std 1, $n=%d$' % (theta,n));plt.grid(True);\n",
    "plt.legend(loc='lower right');plt.xlim(1e-3,1);plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a53231b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Asymptotic Relative Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0de3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Finally, consider a measure mentioned frequently in Conover, and defined in Section 2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387afdc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider two tests w/same significance $\\alpha=\\alpha_1=\\alpha_2$ & power $\\gamma=\\gamma_1=\\gamma_2$ at fixed $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233c6a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To get this, need to apply tests to samples of different sizes $n_1\\ne n_2$. The **relative efficiency** of test 2 relative to test 1 is $\\frac{n_1}{n_2}$. (The more efficient test can acheive the same significance and power with a smaller sample size.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad8ec8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the limit that the sample size becomes large, this is known as the asymptotic relative efficiency (A.R.E.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d92b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Tricky to estimate relative efficiency numerically, since you don't know what sample size to use for each test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c953682",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One trick: fix $\\alpha$ & $\\theta$, and calculate $\\gamma$ for a bunch of sample sizes $n$.  Plot $n$ vs $\\gamma$ for each test & compare $n_1$ & $n_2$ at $\\gamma_1=\\gamma_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a3a83",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For large $n$, $\\gamma\\approx 1$, so actually best to calculate $\\beta=1-\\gamma=P(\\color{royalblue}{T}\\mathbin{\\le}c|H_1)$ & plot $\\beta$ on a logarithmic scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7cad0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Also use a log scale for $n$, so we can eyeball the ratio of different $n$ at the same $\\gamma$, which corresponds to the separation on a log scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50830015",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plot this for the sign test.\n",
    "- $c_s$ is the $1-\\alpha$ quantile of the $\\operatorname{Bin}(n,0.5)$ distribution (i.e., a different threshold for every $n$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b84b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n_n = np.array(np.logspace(2,5,100),dtype=int); n_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705fa66e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.05; p0 = 0.5; cs_n = stats.binom(n_n,p0).isf(alpha); cs_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679f601",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- False dismissal prob $\\beta=1-\\gamma$ is $P(\\color{royalblue}{N_+}\\mathbin{\\le}c_s)$ for $\\color{royalblue}{N_+}\\sim\\operatorname{Bin}(n,p_{\\theta})$, where $p_{\\theta}$ is the\n",
    "fraction of the sampling distribution to the right of the origin when\n",
    "the location parameter is $\\theta$. We do this for a $N(\\theta,1)$\n",
    "sampling distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d350ac3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "theta = 0.2; p1 = stats.norm(loc=theta).sf(0); p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8854785",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "betas_n = stats.binom(n_n,p1).cdf(cs_n); betas_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30de8d2",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.loglog(betas_n,n_n,'-',label='sign test');\n",
    "plt.title(r'Relative Efficiency for $N(%.1f,1)$, $\\alpha=%g$' % (theta,alpha));\n",
    "plt.xlabel(r'$1-\\gamma$');plt.ylabel(r'$n$');plt.grid(True);plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becf302",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now for the $t$-test, we really don’t want to do a Monte Carlo which\n",
    "we’d have to re-do for each sample size $n$ we’re trying out. Instead,\n",
    "we rely on the central limit theorem, which tells us that, for large\n",
    "$n$, the $t$-statistic is approximately a $N(\\theta\\sqrt{n},1)$ random\n",
    "variable, and\n",
    "$$P({\\color{royalblue}{T}}\\le c) \\approx \\Phi\\left(c-\\theta\\sqrt{n}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d4713",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ct = stats.norm.isf(alpha); ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c3c6d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "betat_n = stats.norm(loc=theta*np.sqrt(n_n)).cdf(ct); betat_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce740aaa",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.loglog(betas_n,n_n,'-',label='sign test');\n",
    "plt.loglog(betat_n,n_n,'--',label=r'$t$-test (CLT approx)');\n",
    "plt.title(r'Relative Efficiency for $N(%.1f,1)$, $\\alpha=%g$' % (theta,alpha));\n",
    "plt.xlabel(r'$1-\\gamma$');plt.ylabel(r'$n$');plt.grid(True);plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47837e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For large $n$, separation is constant, ratio $\\approx 6\\times 10^4/4\\times 10^4\\approx 1.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a3093",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that, as the curves go to large $n$ (in the upper left-hand\n",
    "part of the figure), they seem to have a constant difference between\n",
    "them, and by eyeballing the fact that the sign test curve hits the left\n",
    "edge at around $6\\times 10^4$ and the $t$-test curve would be\n",
    "extrapolated to around $4\\times 10^4$, it looks like we might expect the\n",
    "A.R.E. of the $t$-test relative to the sign test to be $1.5$ or so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8148375",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We\n",
    "also notice that neither of the curves actually makes it up to $10^5$\n",
    "because the $\\beta$ calculation is underflowing, and the binomial cdf in\n",
    "the sign test does so sooner than the normal cdf in the $t$ test. This\n",
    "suggests that we should use some sort of asymptotic approximation, and\n",
    "in fact this is used in the actual computation of the limiting case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19afebc0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The calculation of A.R.E. is complicated in general, but it’s made\n",
    "somewhat simpler if we recall that for large sample sizes most test\n",
    "statistics are approximately normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba89329a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$${\\color{royalblue}{Z_s}} = \\frac{{\\color{royalblue}{N_{+}}}-n/2}{\\sqrt{n}/2} \\hbox{ (sign test)}\n",
    "\\qquad\n",
    "{\\color{royalblue}{Z_t}} = \\frac{{\\color{royalblue}{{{\\overline{X}}}}}}{\\sqrt{{\\color{royalblue}{S^2}}/n}}\n",
    "\\hbox{ ($t$-test)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a9ee6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "under $H_0$, both are standard normal, and so we reject if the statistic\n",
    "exceeds $z_{1-\\alpha}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7711d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the location parameter is $\\theta$ under\n",
    "$H_1$, then $E({\\color{royalblue}{N_{+}}})=np_{\\theta}$ and\n",
    "$V({\\color{royalblue}{N_{+}}})=np_{\\theta}(1-p_{\\theta})$, so\n",
    "\n",
    "$$\\begin{gathered}\n",
    "    E({\\color{royalblue}{Z_s}}) = \\frac{np_{\\theta}-n/2}{\\sqrt{n}/2}\n",
    "    = \\sqrt{n}(2p_{\\theta}-1)\n",
    "    \\\\\n",
    "    \\operatorname{Var}({\\color{royalblue}{Z_s}}) = \\frac{4}{n}np_{\\theta}(1-p_{\\theta})\n",
    "    = 4p_{\\theta}(1-p_{\\theta})\n",
    "  \\end{gathered}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871bc50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and $$\\gamma_s \\approx P({\\color{royalblue}{Z_s}} > z_{1-\\alpha})\n",
    "  = 1 - \\Phi\\left(\n",
    "    \\frac{z_{1-\\alpha}-\\sqrt{n_s}(2p_{\\theta}-1)}\n",
    "    {\\sqrt{4p_{\\theta}(1-p_{\\theta})}}\n",
    "  \\right)$$ where\n",
    "$$\\Phi(z) = \\frac{1}{2\\pi}\\int_{-\\infty}^z e^{-t^2/2}\\,dt$$ is the\n",
    "standard normal cdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ef08d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, if the sampling distribution is\n",
    "$N(\\theta,1)$, then $E({\\color{royalblue}{{{\\overline{X}}}}})=\\theta$ and\n",
    "$V({\\color{royalblue}{{{\\overline{X}}}}})=1/n$, and\n",
    "$$E({\\color{royalblue}{Z_t}}) = \\theta\\sqrt{n}\n",
    "  \\qquad\\hbox{and}\\qquad\n",
    "  \\operatorname{Var}({\\color{royalblue}{Z_t}}) = 1$$ so\n",
    "$$\\gamma_t \\approx P({\\color{royalblue}{Z_t}} > z_{1-\\alpha})\n",
    "  = 1 - \\Phi\\left(z_{1-\\alpha}-\\theta\\sqrt{n_t}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583f8a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\gamma_s \\approx 1 - \\Phi\\left(\n",
    "    \\frac{z_{1-\\alpha}-\\sqrt{n_s}(2p_{\\theta}-1)}\n",
    "    {\\sqrt{4p_{\\theta}(1-p_{\\theta})}}\n",
    "  \\right)\\qquad\\hbox{and}\\qquad\\gamma_t \\approx 1 - \\Phi\\left(z_{1-\\alpha}-\\theta\\sqrt{n_t}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ba19d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, in order\n",
    "to ensure that $\\gamma_s=\\gamma_t$, we must have\n",
    "$$\\frac{\\sqrt{n_s}(2p_{\\theta}-1)-z_{1-\\alpha}}\n",
    "  {\\sqrt{4p_{\\theta}(1-p_{\\theta})}}\n",
    "  \\approx \\theta\\sqrt{n_t}-z_{1-\\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c2f19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It seems like in general\n",
    "$n_s/n_t$ will depend on both $\\theta$ and $\\alpha$, and the $\\theta$\n",
    "dependence doesn’t look like it will go away even when\n",
    "$n\\rightarrow\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98233d60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, since we’re supposed to carry out the\n",
    "comparison at fixed $\\alpha$ and $\\gamma$, as we send the sample size to\n",
    "infinity, we have to send the effect size $\\theta$ to zero, which means\n",
    "$p_{\\theta}\\rightarrow \\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b7c4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus we can (to leading order) set\n",
    "the denominator on the left-hand size to one, which leaves us with\n",
    "$$\\sqrt{n_s}(2p_{\\theta}-1)-z_{1-\\alpha}\n",
    "  \\approx \\theta\\sqrt{n_t}-z_{1-\\alpha}$$ and\n",
    "$$\\frac{n_s}{n_t} = \\lim_{\\theta\\rightarrow 0}\n",
    "  \\left(\\frac{\\theta}{2p_{\\theta}-1}\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64cd92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can use l’Hôpital’s\n",
    "rule to deal with the limit, or just expand the form of $p_\\theta$ to\n",
    "first order for small $\\theta$. If the sampling distribution is normal,\n",
    "$$p_{\\theta}\n",
    "    =\\frac{1}{\\sqrt{2\\pi}}\\int_{0}^{\\infty} e^{-(x-\\theta)^2/2}\\,dx\n",
    "    =\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\theta} e^{-z^2/2}\\,dz\n",
    "\\approx \\frac{1}{2} + \\frac{\\theta}{\\sqrt{2\\pi}} e^{-0^2/2}$$ so\n",
    "$$\\lim_{\\theta\\rightarrow 0} \\frac{\\theta}{2p_{\\theta}-1}\n",
    "  = \\lim_{\\theta\\rightarrow 0} \\frac{\\theta}{2\\theta/\\sqrt{2\\pi}}\n",
    "  = \\sqrt{\\frac{\\pi}{2}}$$ and\n",
    "$\\hbox{A.R.E.} = \\frac{\\pi}{2} \\approx 1.57$ which is about the value\n",
    "we eyeballed from the plot."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
